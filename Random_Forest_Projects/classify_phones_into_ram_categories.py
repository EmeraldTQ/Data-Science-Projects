# -*- coding: utf-8 -*-
"""Classify_phones_into_RAM_categories.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W3JZf2eih4rxFxH1FZ34ZKkM-MElvx1t

##Steps included for Your (cellphone_test.csv) dataset notebook

1. Load the dataset
2. Check for missing values
3. Handle missing data
4. Perform EDA (visualizations, statistical insights)
5. Feature engineering (creating new features, encoding categorical variables, etc.)

###Step 1. Install Required Libraries
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""###Step 2. Load the Dataset"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Machine_Learning/Datasets/CellPhone_test.csv")

df.head()

"""###Step 3. Check Dataset Info"""

# Check for missing values & data types
df.info()

df.drop(columns=['id'], inplace=True)

df.shape

# Get numerical statistics
df.describe()

"""###Step 4. Handle missing data"""

# Check for missing values
df.isnull().sum()

"""###Step 4.1. Handle duplicate data"""

df[df.duplicated]

#If there're duplicated data, perform this step
df.drop_duplicates(inplace=True, keep="first")
df.shape #The output shape will be the same as before as there're any duplicated data

"""###Step 4.2. If there's null value, perfomring these steps to fill the missing data"""

# Fill missing numeric values with median
df.fillna(df.median(numeric_only=True), inplace=True)

# Fill categorical missing values with mode
for col in df.select_dtypes(include=['object']).columns:
    df[col].fillna(df[col].mode()[0], inplace=True)

"""###Step 5. Data Visualization"""

# Plot distribution of numerical features
df.hist(figsize=(10, 15), bins=20)
plt.show()

"""###Step 5.1. Detecting Outliers"""

# Boxplot to detect outliers
plt.figure(figsize=(8, 3))
sns.boxplot(data=df)
plt.xticks(rotation=90)
plt.show()

"""###Step 5.2. Finding out correlation"""

# Correlation heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.show()

"""###Step 6. Data Preprocessing

####Step 6.1. Drop Unnecessary Columns
"""

columns_to_drop = ['fc', 'talk_time', 'sc_w']
df_cleaned = df.drop(columns=columns_to_drop)

print(f"Remaining columns: {df_cleaned.columns}")

"""####Step 6.2. Apply Log Transformation for Highly Skewed Features"""

import numpy as np

# Log transformation for pixel height & width
df_cleaned['px_height'] = np.log1p(df_cleaned['px_height'])  # log1p handles log(0)
df_cleaned['px_width'] = np.log1p(df_cleaned['px_width'])

"""####Step 6.3. Cap Outliers (Winsorization) for ram"""

# Apply Winsorization (capping) at 5th and 95th percentiles
lower_limit = df_cleaned['ram'].quantile(0.05)
upper_limit = df_cleaned['ram'].quantile(0.95)

df_cleaned['ram'] = np.clip(df_cleaned['ram'], lower_limit, upper_limit)

"""####Step 6.4. Scale Numerical Features"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

# Select numerical columns for scaling
numerical_cols = ['battery_power', 'clock_speed', 'int_memory', 'm_dep', 'px_height', 'px_width', 'ram']
df_cleaned[numerical_cols] = scaler.fit_transform(df_cleaned[numerical_cols])

df_cleaned.head()

df.ram

"""####Step 6.5. Encode Binary Categorical Features"""

# Convert binary categorical features to integers (0/1)
binary_cols = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']
df_cleaned[binary_cols] = df_cleaned[binary_cols].astype(int)

"""###Step 7. Categorize ram into Classes"""

df_cleaned['ram_category'] = pd.cut(
    df_cleaned['ram'],
    bins=[0, 0.33, 0.66, 1],  # Divide 0-1 range into 3 equal parts
    labels=[0, 1, 2],
    include_lowest=True
)

df_cleaned['ram_category'].value_counts()

"""###Step 8. Define Features & Target

X contains only input features, and y contains the RAM category labels.


"""

# Drop the original RAM column (since we already categorized it)
X = df_cleaned.drop(columns=['ram_category'])  # Features
y = df_cleaned['ram_category']  # Target variable

"""###Step 9. Split Data for ML Training"""

from sklearn.model_selection import train_test_split

# Split dataset (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape}, Testing set size: {X_test.shape}")

"""###Step 9.1. Train a Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize and train the model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt

# Plot class distribution
sns.countplot(x=y, palette='viridis')
plt.title("RAM Category Distribution")
plt.xlabel("RAM Category")
plt.ylabel("Count")
plt.show()

"""###Step 9.2. Fixing overfitting issue"""

model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy_score(y_test, y_pred)

from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(model, X, y, cv=5)
print("Cross-validation accuracy:", cv_scores.mean())

import pandas as pd

# Get feature importances
feature_importance = pd.Series(model.feature_importances_, index=X.columns)
feature_importance.sort_values(ascending=False)

from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression()
log_model.fit(X_train, y_train)

y_pred_log = log_model.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_log))

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(model, X, y, cv=5)
print("Cross-validation accuracy:", cv_scores.mean())

# Drop 'ram' from features
X = X.drop(columns=['ram'])

# Split the new dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Updated training set size:", X_train.shape)
print("Updated test set size:", X_test.shape)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Train the model again without RAM
model = RandomForestClassifier(n_estimators=50, max_depth=8, min_samples_split=5, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate performance
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression

log_model = LogisticRegression(max_iter=1000)  # Increased iterations
log_model.fit(X_train, y_train)

y_pred_log = log_model.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_log))

from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(model, X, y, cv=5)
print("Cross-validation accuracy:", cv_scores.mean())